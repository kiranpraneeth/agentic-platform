"""Add RAG tables and pgvector extension

Revision ID: 583707ecf3d3
Revises: 6266ca9c8ac3
Create Date: 2025-10-19 04:26:54.366083

"""
from typing import Sequence, Union

from alembic import op
import sqlalchemy as sa
import pgvector.sqlalchemy


# revision identifiers, used by Alembic.
revision: str = '583707ecf3d3'
down_revision: Union[str, None] = '6266ca9c8ac3'
branch_labels: Union[str, Sequence[str], None] = None
depends_on: Union[str, Sequence[str], None] = None


def upgrade() -> None:
    """Upgrade database schema."""
    # Enable pgvector extension
    op.execute('CREATE EXTENSION IF NOT EXISTS vector')

    # ### commands auto generated by Alembic - please adjust! ###
    op.create_table('collections',
    sa.Column('tenant_id', sa.UUID(), nullable=False),
    sa.Column('name', sa.String(length=255), nullable=False),
    sa.Column('description', sa.Text(), nullable=True),
    sa.Column('embedding_model', sa.String(length=100), nullable=False),
    sa.Column('embedding_dimensions', sa.Integer(), nullable=False),
    sa.Column('chunk_size', sa.Integer(), nullable=False),
    sa.Column('chunk_overlap', sa.Integer(), nullable=False),
    sa.Column('visibility', sa.String(length=50), nullable=False),
    sa.Column('document_count', sa.Integer(), nullable=False),
    sa.Column('chunk_count', sa.Integer(), nullable=False),
    sa.Column('config', sa.JSON(), nullable=True),
    sa.Column('extra_metadata', sa.JSON(), nullable=True),
    sa.Column('id', sa.UUID(), nullable=False),
    sa.Column('created_at', sa.DateTime(timezone=True), server_default=sa.text('now()'), nullable=False),
    sa.Column('updated_at', sa.DateTime(timezone=True), server_default=sa.text('now()'), nullable=False),
    sa.Column('deleted_at', sa.DateTime(timezone=True), nullable=True),
    sa.ForeignKeyConstraint(['tenant_id'], ['tenants.id'], ondelete='CASCADE'),
    sa.PrimaryKeyConstraint('id')
    )
    op.create_index('idx_collections_tenant_name', 'collections', ['tenant_id', 'name'], unique=False)
    op.create_index('idx_collections_visibility', 'collections', ['visibility'], unique=False)
    op.create_index(op.f('ix_collections_tenant_id'), 'collections', ['tenant_id'], unique=False)
    op.create_table('documents',
    sa.Column('tenant_id', sa.UUID(), nullable=False),
    sa.Column('collection_id', sa.UUID(), nullable=False),
    sa.Column('title', sa.String(length=500), nullable=False),
    sa.Column('source', sa.String(length=1000), nullable=True),
    sa.Column('content_type', sa.String(length=100), nullable=False),
    sa.Column('status', sa.String(length=50), nullable=False),
    sa.Column('file_size_bytes', sa.Integer(), nullable=True),
    sa.Column('file_hash', sa.String(length=64), nullable=True),
    sa.Column('storage_path', sa.String(length=1000), nullable=True),
    sa.Column('chunk_count', sa.Integer(), nullable=False),
    sa.Column('total_tokens', sa.Integer(), nullable=True),
    sa.Column('processed_at', sa.DateTime(timezone=True), nullable=True),
    sa.Column('error_message', sa.Text(), nullable=True),
    sa.Column('extra_metadata', sa.JSON(), nullable=True),
    sa.Column('id', sa.UUID(), nullable=False),
    sa.Column('created_at', sa.DateTime(timezone=True), server_default=sa.text('now()'), nullable=False),
    sa.Column('updated_at', sa.DateTime(timezone=True), server_default=sa.text('now()'), nullable=False),
    sa.Column('deleted_at', sa.DateTime(timezone=True), nullable=True),
    sa.ForeignKeyConstraint(['collection_id'], ['collections.id'], ondelete='CASCADE'),
    sa.ForeignKeyConstraint(['tenant_id'], ['tenants.id'], ondelete='CASCADE'),
    sa.PrimaryKeyConstraint('id')
    )
    op.create_index('idx_documents_collection_status', 'documents', ['collection_id', 'status'], unique=False)
    op.create_index('idx_documents_tenant_collection', 'documents', ['tenant_id', 'collection_id'], unique=False)
    op.create_index(op.f('ix_documents_collection_id'), 'documents', ['collection_id'], unique=False)
    op.create_index(op.f('ix_documents_status'), 'documents', ['status'], unique=False)
    op.create_index(op.f('ix_documents_tenant_id'), 'documents', ['tenant_id'], unique=False)
    op.create_table('chunks',
    sa.Column('tenant_id', sa.UUID(), nullable=False),
    sa.Column('document_id', sa.UUID(), nullable=False),
    sa.Column('collection_id', sa.UUID(), nullable=False),
    sa.Column('content', sa.Text(), nullable=False),
    sa.Column('embedding', pgvector.sqlalchemy.Vector(dim=1536), nullable=True),
    sa.Column('sequence_number', sa.Integer(), nullable=False),
    sa.Column('token_count', sa.Integer(), nullable=True),
    sa.Column('start_char', sa.Integer(), nullable=True),
    sa.Column('end_char', sa.Integer(), nullable=True),
    sa.Column('page_number', sa.Integer(), nullable=True),
    sa.Column('section', sa.String(length=255), nullable=True),
    sa.Column('extra_metadata', sa.JSON(), nullable=True),
    sa.Column('created_at', sa.DateTime(timezone=True), server_default='now()', nullable=False),
    sa.Column('id', sa.UUID(), nullable=False),
    sa.ForeignKeyConstraint(['collection_id'], ['collections.id'], ondelete='CASCADE'),
    sa.ForeignKeyConstraint(['document_id'], ['documents.id'], ondelete='CASCADE'),
    sa.ForeignKeyConstraint(['tenant_id'], ['tenants.id'], ondelete='CASCADE'),
    sa.PrimaryKeyConstraint('id')
    )
    op.create_index('idx_chunks_collection', 'chunks', ['collection_id'], unique=False)
    op.create_index('idx_chunks_document_sequence', 'chunks', ['document_id', 'sequence_number'], unique=False)
    op.create_index('idx_chunks_embedding_hnsw', 'chunks', ['embedding'], unique=False, postgresql_using='hnsw', postgresql_with={'m': 16, 'ef_construction': 64}, postgresql_ops={'embedding': 'vector_cosine_ops'})
    op.create_index(op.f('ix_chunks_collection_id'), 'chunks', ['collection_id'], unique=False)
    op.create_index(op.f('ix_chunks_created_at'), 'chunks', ['created_at'], unique=False)
    op.create_index(op.f('ix_chunks_document_id'), 'chunks', ['document_id'], unique=False)
    op.create_index(op.f('ix_chunks_tenant_id'), 'chunks', ['tenant_id'], unique=False)
    op.create_table('rag_queries',
    sa.Column('tenant_id', sa.UUID(), nullable=False),
    sa.Column('collection_id', sa.UUID(), nullable=True),
    sa.Column('conversation_id', sa.UUID(), nullable=True),
    sa.Column('query_text', sa.Text(), nullable=False),
    sa.Column('query_embedding', pgvector.sqlalchemy.Vector(dim=1536), nullable=True),
    sa.Column('top_k', sa.Integer(), nullable=False),
    sa.Column('similarity_threshold', sa.Float(), nullable=True),
    sa.Column('metadata_filter', sa.JSON(), nullable=True),
    sa.Column('result_count', sa.Integer(), nullable=False),
    sa.Column('results', sa.JSON(), nullable=True),
    sa.Column('retrieval_time_ms', sa.Integer(), nullable=True),
    sa.Column('feedback_score', sa.Integer(), nullable=True),
    sa.Column('feedback_text', sa.Text(), nullable=True),
    sa.Column('extra_metadata', sa.JSON(), nullable=True),
    sa.Column('created_at', sa.DateTime(timezone=True), server_default='now()', nullable=False),
    sa.Column('id', sa.UUID(), nullable=False),
    sa.ForeignKeyConstraint(['collection_id'], ['collections.id'], ondelete='CASCADE'),
    sa.ForeignKeyConstraint(['conversation_id'], ['conversations.id'], ondelete='SET NULL'),
    sa.ForeignKeyConstraint(['tenant_id'], ['tenants.id'], ondelete='CASCADE'),
    sa.PrimaryKeyConstraint('id')
    )
    op.create_index('idx_rag_queries_created_at', 'rag_queries', ['created_at'], unique=False, postgresql_using='btree')
    op.create_index('idx_rag_queries_tenant_collection', 'rag_queries', ['tenant_id', 'collection_id'], unique=False)
    op.create_index(op.f('ix_rag_queries_collection_id'), 'rag_queries', ['collection_id'], unique=False)
    op.create_index(op.f('ix_rag_queries_conversation_id'), 'rag_queries', ['conversation_id'], unique=False)
    op.create_index(op.f('ix_rag_queries_created_at'), 'rag_queries', ['created_at'], unique=False)
    op.create_index(op.f('ix_rag_queries_tenant_id'), 'rag_queries', ['tenant_id'], unique=False)
    # ### end Alembic commands ###


def downgrade() -> None:
    """Downgrade database schema."""
    # ### commands auto generated by Alembic - please adjust! ###
    op.drop_index(op.f('ix_rag_queries_tenant_id'), table_name='rag_queries')
    op.drop_index(op.f('ix_rag_queries_created_at'), table_name='rag_queries')
    op.drop_index(op.f('ix_rag_queries_conversation_id'), table_name='rag_queries')
    op.drop_index(op.f('ix_rag_queries_collection_id'), table_name='rag_queries')
    op.drop_index('idx_rag_queries_tenant_collection', table_name='rag_queries')
    op.drop_index('idx_rag_queries_created_at', table_name='rag_queries', postgresql_using='btree')
    op.drop_table('rag_queries')
    op.drop_index(op.f('ix_chunks_tenant_id'), table_name='chunks')
    op.drop_index(op.f('ix_chunks_document_id'), table_name='chunks')
    op.drop_index(op.f('ix_chunks_created_at'), table_name='chunks')
    op.drop_index(op.f('ix_chunks_collection_id'), table_name='chunks')
    op.drop_index('idx_chunks_embedding_hnsw', table_name='chunks', postgresql_using='hnsw', postgresql_with={'m': 16, 'ef_construction': 64}, postgresql_ops={'embedding': 'vector_cosine_ops'})
    op.drop_index('idx_chunks_document_sequence', table_name='chunks')
    op.drop_index('idx_chunks_collection', table_name='chunks')
    op.drop_table('chunks')
    op.drop_index(op.f('ix_documents_tenant_id'), table_name='documents')
    op.drop_index(op.f('ix_documents_status'), table_name='documents')
    op.drop_index(op.f('ix_documents_collection_id'), table_name='documents')
    op.drop_index('idx_documents_tenant_collection', table_name='documents')
    op.drop_index('idx_documents_collection_status', table_name='documents')
    op.drop_table('documents')
    op.drop_index(op.f('ix_collections_tenant_id'), table_name='collections')
    op.drop_index('idx_collections_visibility', table_name='collections')
    op.drop_index('idx_collections_tenant_name', table_name='collections')
    op.drop_table('collections')
    # ### end Alembic commands ###
